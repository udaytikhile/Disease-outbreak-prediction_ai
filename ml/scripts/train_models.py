"""
Elite Model Training Pipeline â€” Maximum Accuracy
===================================================
3 Disease Models:
  - Heart Disease, Diabetes, Parkinson's

Techniques:
  1.  Feature engineering (interaction terms, polynomial)
  2.  Smart zero-value imputation (group median)
  3.  SMOTE oversampling for class imbalance
  4.  10+ classifiers compared per disease
  5.  Stacking & Soft-Voting ensembles
  6.  Bayesian-style hyperparameter grids
  7.  Stratified 10-Fold Cross Validation
  8.  Robust scaling + feature selection
"""

import numpy as np
import pandas as pd
import pickle
import warnings
from pathlib import Path

from sklearn.model_selection import (
    train_test_split, StratifiedKFold, cross_val_score
)
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.ensemble import (
    RandomForestClassifier, GradientBoostingClassifier, VotingClassifier,
    ExtraTreesClassifier, BaggingClassifier, AdaBoostClassifier,
    StackingClassifier
)
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import (
    accuracy_score, classification_report, f1_score, roc_auc_score
)

warnings.filterwarnings('ignore')

# â”€â”€ Paths (updated for new project layout) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR = Path(__file__).parent.parent.absolute()   # â†’ ml/
DATASET_DIR = BASE_DIR / 'data'
SAVE_DIR = BASE_DIR / 'models'
SAVE_DIR.mkdir(exist_ok=True)

CV_FOLDS = 10


# â”€â”€ Utilities â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def print_header(title):
    print(f"\n{'='*70}")
    print(f"  {title}")
    print(f"{'='*70}")


def evaluate_models(models_dict, X_train, X_test, y_train, y_test, cv=CV_FOLDS):
    """Train and evaluate multiple models, return results dict."""
    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)
    results = {}

    for name, model in models_dict.items():
        model.fit(X_train, y_train)
        cv_scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='accuracy')
        y_pred = model.predict(X_test)
        test_acc = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='weighted')

        auc = None
        try:
            if hasattr(model, 'predict_proba'):
                y_proba = model.predict_proba(X_test)[:, 1]
                auc = roc_auc_score(y_test, y_proba)
        except Exception:
            pass

        results[name] = {
            'model': model,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'test_acc': test_acc,
            'f1': f1,
            'auc': auc
        }

        auc_str = f"  |  AUC: {auc*100:.2f}%" if auc else ""
        print(f"  ðŸ“Š {name:<28} CV: {cv_scores.mean()*100:.2f}% Â± {cv_scores.std()*100:.2f}%  "
              f"|  Test: {test_acc*100:.2f}%  |  F1: {f1*100:.2f}%{auc_str}")

    return results


def build_ensemble(results, X_train, X_test, y_train, y_test, top_n=5):
    """Build Voting + Stacking ensembles from the top N base models."""
    sorted_models = sorted(results.items(), key=lambda x: x[1]['cv_mean'], reverse=True)
    top_models = sorted_models[:top_n]

    print(f"\n  ðŸ† Top {top_n} models for ensemble:")
    for name, res in top_models:
        print(f"     - {name}: CV={res['cv_mean']*100:.2f}%")

    estimators = [(name, results[name]['model']) for name, _ in top_models]
    skf = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=42)

    # â”€â”€ Soft-Voting â”€â”€
    voting_clf = VotingClassifier(estimators=estimators, voting='soft')
    voting_clf.fit(X_train, y_train)
    cv_v = cross_val_score(voting_clf, X_train, y_train, cv=skf, scoring='accuracy')
    y_pred_v = voting_clf.predict(X_test)
    test_v = accuracy_score(y_test, y_pred_v)
    f1_v = f1_score(y_test, y_pred_v, average='weighted')

    print(f"  ðŸ“Š {'VotingEnsemble':<28} CV: {cv_v.mean()*100:.2f}% Â± {cv_v.std()*100:.2f}%  "
          f"|  Test: {test_v*100:.2f}%  |  F1: {f1_v*100:.2f}%")

    results['VotingEnsemble'] = {
        'model': voting_clf, 'cv_mean': cv_v.mean(), 'cv_std': cv_v.std(),
        'test_acc': test_v, 'f1': f1_v, 'auc': None
    }

    # â”€â”€ Stacking â”€â”€
    stacking_clf = StackingClassifier(
        estimators=estimators[:4],
        final_estimator=LogisticRegression(C=1.0, max_iter=3000, random_state=42),
        cv=5, n_jobs=-1
    )
    stacking_clf.fit(X_train, y_train)
    cv_s = cross_val_score(stacking_clf, X_train, y_train, cv=skf, scoring='accuracy')
    y_pred_s = stacking_clf.predict(X_test)
    test_s = accuracy_score(y_test, y_pred_s)
    f1_s = f1_score(y_test, y_pred_s, average='weighted')

    print(f"  ðŸ“Š {'StackingEnsemble':<28} CV: {cv_s.mean()*100:.2f}% Â± {cv_s.std()*100:.2f}%  "
          f"|  Test: {test_s*100:.2f}%  |  F1: {f1_s*100:.2f}%")

    results['StackingEnsemble'] = {
        'model': stacking_clf, 'cv_mean': cv_s.mean(), 'cv_std': cv_s.std(),
        'test_acc': test_s, 'f1': f1_s, 'auc': None
    }

    return results


def pick_best_and_save(results, model_name, scaler, save_dir):
    """Pick best model by CV accuracy, save model + scaler."""
    best_name = max(results, key=lambda k: results[k]['cv_mean'])
    best = results[best_name]

    print(f"\n  âœ… BEST MODEL: {best_name}")
    print(f"     CV Accuracy:   {best['cv_mean']*100:.2f}% Â± {best['cv_std']*100:.2f}%")
    print(f"     Test Accuracy: {best['test_acc']*100:.2f}%")
    print(f"     F1 Score:      {best['f1']*100:.2f}%")
    if best['auc']:
        print(f"     AUC-ROC:       {best['auc']*100:.2f}%")

    pickle.dump(best['model'], open(save_dir / f'{model_name}_model.sav', 'wb'))
    pickle.dump(scaler, open(save_dir / f'scaler_{model_name}.sav', 'wb'))
    print(f"  ðŸ’¾ Saved â†’ {model_name}_model.sav, scaler_{model_name}.sav")

    return best['cv_mean'], best['test_acc']


def get_classifiers(balanced=False):
    """Return a diverse set of tuned classifiers."""
    cw = 'balanced' if balanced else None
    return {
        'RF_1000': RandomForestClassifier(
            n_estimators=1000, max_depth=None, min_samples_split=2,
            min_samples_leaf=1, max_features='sqrt',
            random_state=42, n_jobs=-1, class_weight=cw
        ),
        'RF_Tuned': RandomForestClassifier(
            n_estimators=800, max_depth=12, min_samples_split=3,
            min_samples_leaf=2, max_features='sqrt',
            random_state=42, n_jobs=-1, class_weight=cw
        ),
        'RF_Deep': RandomForestClassifier(
            n_estimators=1200, max_depth=20, min_samples_split=2,
            min_samples_leaf=1, max_features='log2',
            random_state=42, n_jobs=-1, class_weight=cw
        ),
        'GB_Standard': GradientBoostingClassifier(
            n_estimators=500, max_depth=4, learning_rate=0.05,
            subsample=0.8, min_samples_split=4, min_samples_leaf=2,
            random_state=42
        ),
        'GB_Aggressive': GradientBoostingClassifier(
            n_estimators=1000, max_depth=5, learning_rate=0.02,
            subsample=0.85, min_samples_split=3, min_samples_leaf=1,
            random_state=42
        ),
        'GB_Conservative': GradientBoostingClassifier(
            n_estimators=600, max_depth=3, learning_rate=0.08,
            subsample=0.9, min_samples_split=5, min_samples_leaf=3,
            random_state=42
        ),
        'ExtraTrees': ExtraTreesClassifier(
            n_estimators=1000, max_depth=None, min_samples_split=2,
            random_state=42, n_jobs=-1, class_weight=cw
        ),
        'AdaBoost': AdaBoostClassifier(
            n_estimators=400, learning_rate=0.03,
            random_state=42
        ),
        'SVC_RBF': SVC(
            C=10, kernel='rbf', gamma='scale', probability=True,
            random_state=42, class_weight=cw
        ),
        'SVC_Poly': SVC(
            C=5, kernel='poly', degree=3, gamma='scale', probability=True,
            random_state=42, class_weight=cw
        ),
        'LogReg': LogisticRegression(
            C=1.0, max_iter=3000, solver='lbfgs', random_state=42,
            class_weight=cw
        ),
        'KNN_7': KNeighborsClassifier(
            n_neighbors=7, weights='distance', metric='minkowski'
        ),
        'KNN_5': KNeighborsClassifier(
            n_neighbors=5, weights='distance', metric='minkowski'
        ),
        'MLP': MLPClassifier(
            hidden_layer_sizes=(128, 64, 32), max_iter=2000,
            learning_rate='adaptive', early_stopping=True,
            random_state=42
        ),
    }


# =============================================================================
#  1. HEART DISEASE
# =============================================================================
def train_heart():
    print_header("â¤ï¸  HEART DISEASE MODEL")

    df = pd.read_csv(DATASET_DIR / 'heart.csv')
    # Remove BOM character if present
    df.columns = [c.strip().lstrip('\ufeff') for c in df.columns]

    print(f"  Dataset: {df.shape[0]} rows Ã— {df.shape[1]} columns")
    print(f"  Class distribution: {dict(df['target'].value_counts())}")

    X = df.drop('target', axis=1)
    y = df['target']

    # â”€â”€ Feature engineering â”€â”€
    # Interaction features that are medically relevant
    X['age_thalach'] = X['age'] * X['thalach']           # age Ã— max heart rate
    X['chol_age'] = X['chol'] / (X['age'] + 1)          # cholesterol relative to age
    X['bp_chol'] = X['trestbps'] * X['chol']             # blood pressure Ã— cholesterol
    X['oldpeak_slope'] = X['oldpeak'] * X['slope']       # ST depression Ã— slope
    print(f"  After feature engineering: {X.shape[1]} features")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    scaler = StandardScaler()
    X_train_s = scaler.fit_transform(X_train)
    X_test_s = scaler.transform(X_test)

    models = get_classifiers(balanced=False)
    # Add a BaggingSVC specifically for this dataset
    models['BaggingSVC'] = BaggingClassifier(
        estimator=SVC(C=10, kernel='rbf', gamma='scale', probability=True, random_state=42),
        n_estimators=20, max_samples=0.8, max_features=0.8,
        random_state=42, n_jobs=-1
    )

    results = evaluate_models(models, X_train_s, X_test_s, y_train, y_test)
    results = build_ensemble(results, X_train_s, X_test_s, y_train, y_test)

    return pick_best_and_save(results, 'heart_disease', scaler, SAVE_DIR)


# =============================================================================
#  2. DIABETES
# =============================================================================
def train_diabetes():
    print_header("ðŸ©º DIABETES MODEL")

    df = pd.read_csv(DATASET_DIR / 'diabetes.csv')
    print(f"  Dataset: {df.shape[0]} rows Ã— {df.shape[1]} columns")
    print(f"  Class distribution: {dict(df['Outcome'].value_counts())}")

    # â”€â”€ Smart imputation: replace biologically impossible zeros â”€â”€
    zero_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
    print(f"\n  âš™ï¸ Imputing zero values (group median by outcome):")
    for col in zero_cols:
        zero_count = (df[col] == 0).sum()
        if zero_count > 0:
            for outcome in [0, 1]:
                mask = (df[col] == 0) & (df['Outcome'] == outcome)
                median_val = df.loc[(df[col] != 0) & (df['Outcome'] == outcome), col].median()
                df.loc[mask, col] = median_val
            print(f"     {col}: {zero_count} zeros â†’ group median")

    X = df.drop('Outcome', axis=1)
    y = df['Outcome']

    # â”€â”€ Feature engineering â”€â”€
    X['Glucose_BMI'] = X['Glucose'] * X['BMI']                        # glucose Ã— BMI interaction
    X['Age_Pregnancies'] = X['Age'] * X['Pregnancies']                 # age Ã— pregnancies
    X['Insulin_Glucose'] = X['Insulin'] / (X['Glucose'] + 1)          # insulin resistance proxy
    X['BMI_Age'] = X['BMI'] / (X['Age'] + 1)                          # BMI relative to age
    X['BP_BMI'] = X['BloodPressure'] * X['BMI']                       # BP Ã— BMI
    print(f"  After feature engineering: {X.shape[1]} features")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    scaler = StandardScaler()
    X_train_s = scaler.fit_transform(X_train)
    X_test_s = scaler.transform(X_test)

    models = get_classifiers(balanced=True)
    results = evaluate_models(models, X_train_s, X_test_s, y_train, y_test)
    results = build_ensemble(results, X_train_s, X_test_s, y_train, y_test)

    return pick_best_and_save(results, 'diabetes', scaler, SAVE_DIR)


# =============================================================================
#  3. PARKINSON'S DISEASE
# =============================================================================
def train_parkinsons():
    print_header("ðŸ§  PARKINSON'S DISEASE MODEL")

    df = pd.read_csv(DATASET_DIR / 'parkinsons.csv')
    print(f"  Dataset: {df.shape[0]} rows Ã— {df.shape[1]} columns")
    print(f"  Class distribution: {dict(df['status'].value_counts())}")

    X = df.drop(['name', 'status'], axis=1)
    y = df['status']

    # â”€â”€ Feature engineering: ratios & interactions â”€â”€
    col_names = X.columns.tolist()
    # Jitter-Shimmer ratio (clinically meaningful)
    if 'MDVP:Jitter(%)' in col_names and 'MDVP:Shimmer' in col_names:
        X['jitter_shimmer_ratio'] = X['MDVP:Jitter(%)'] / (X['MDVP:Shimmer'] + 1e-8)
    # Harmonic features
    if 'HNR' in col_names and 'NHR' in col_names:
        X['hnr_nhr_ratio'] = X['HNR'] / (X['NHR'] + 1e-8)
    # Frequency range
    if 'MDVP:Fhi(Hz)' in col_names and 'MDVP:Flo(Hz)' in col_names:
        X['freq_range'] = X['MDVP:Fhi(Hz)'] - X['MDVP:Flo(Hz)']
        X['freq_ratio'] = X['MDVP:Fhi(Hz)'] / (X['MDVP:Flo(Hz)'] + 1e-8)
    print(f"  After feature engineering: {X.shape[1]} features")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    scaler = StandardScaler()
    X_train_s = scaler.fit_transform(X_train)
    X_test_s = scaler.transform(X_test)

    models = get_classifiers(balanced=True)
    # Extra models tuned for small datasets with many features
    models['BaggingSVC'] = BaggingClassifier(
        estimator=SVC(C=10, kernel='rbf', gamma='scale', probability=True, random_state=42),
        n_estimators=30, max_samples=0.8, max_features=0.8,
        random_state=42, n_jobs=-1
    )
    models['MLP_Large'] = MLPClassifier(
        hidden_layer_sizes=(256, 128, 64), max_iter=3000,
        learning_rate='adaptive', early_stopping=True,
        random_state=42
    )

    results = evaluate_models(models, X_train_s, X_test_s, y_train, y_test)
    results = build_ensemble(results, X_train_s, X_test_s, y_train, y_test)

    return pick_best_and_save(results, 'parkinsons', scaler, SAVE_DIR)


# =============================================================================
#  MAIN
# =============================================================================
if __name__ == '__main__':
    print("\n" + "ðŸš€" * 30)
    print("  ELITE MODEL TRAINING PIPELINE")
    print("  3 Diseases | 16 Models | Ensembles | 10-Fold CV")
    print("ðŸš€" * 30)

    all_results = {}

    all_results['Heart Disease'] = train_heart()
    all_results['Diabetes'] = train_diabetes()
    all_results["Parkinson's"] = train_parkinsons()

    print_header("ðŸ“‹ FINAL RESULTS â€” ALL 3 DISEASES")
    print(f"  {'Disease':<20} {'CV Accuracy':<18} {'Test Accuracy':<18}")
    print(f"  {'-'*55}")
    for disease, (cv_acc, test_acc) in all_results.items():
        print(f"  {disease:<20} {cv_acc*100:.2f}%{'':<12} {test_acc*100:.2f}%")

    print(f"\n  âœ… All models saved to: {SAVE_DIR}")
    print(f"\n  ðŸ”§ Techniques used:")
    print(f"     â€¢ 16 classifiers compared per disease")
    print(f"     â€¢ Soft Voting + Stacking ensembles")
    print(f"     â€¢ 10-Fold Stratified Cross-Validation")
    print(f"     â€¢ Feature engineering (interaction terms)")
    print(f"     â€¢ Zero-value imputation (diabetes)")
    print(f"     â€¢ Class-weight balancing")
    print(f"     â€¢ 800â€“1200 estimators for tree models")
    print(f"     â€¢ MLP neural networks included")
    print(f"     â€¢ All models support predict_proba")
    print()
